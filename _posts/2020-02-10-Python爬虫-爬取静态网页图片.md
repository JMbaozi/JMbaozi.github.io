---
layout: post
title: 'Python爬虫-爬取静态网页图片'
subtitle: '爬取妹子图网站'
date: 2020-02-10
categories: 技术
tags: Python 技术
---

>我在之前出于兴趣学了一点Python的语法基础，但是没有深入，这里我放出我学习的[项目](https://github.com/jackfrued/Python-100-Days)，作为初学者，可以按照这个教程学习基本的知识。几天前，我刷到了一篇简单的爬虫入门Blog，现在先写一篇关于爬取静态网页图片的Blog，供大家学习，同时我在原博客的基础上进行了修改和添加来更好的理解。

### 代码

```python
"""
爬虫学习
https://blog.csdn.net/namespace_Pt/article/details/104124954
Part 1：
爬取静态网页图片（https://www.mzitu.com/221136）
"""

import requests
import lxml
from bs4 import BeautifulSoup	#引入库
	
dirname = 'picture'		#创建的文件名，图片将保存在子文件夹下

url = 'https://www.mzitu.com/221136'
#header = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:66.0) 			Gecko/20100101 Firefox/66.0",
#"Referer":"https://www.mzitu.com/jiepai/comment-page-1/"}
header = {
          "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.78 Safari/537.36 Edg/80.0.361.45",
          "Referer":"https://www.mzitu.com/jiepai/comment-page-1/"
          }#header设置：https://blog.csdn.net/qq_42787271/article/details/81571229
           #Referer介绍：https://blog.csdn.net/shenqueying/article/details/79426884
#header里必须设置Referer属性，否则无法下载图片
response = requests.get(url,headers = header)	#请求网页
content = response.content
bsobj = BeautifulSoup(content,'lxml')     #解析html

#get_text()方法获取中文字样，用string属性也可以
title = bsobj.find('h2',class_ = 'main-title').get_text()
#按照分析出存储页数上界的位置寻找，存储其string属性即得最大页数
picture_max = bsobj.find('div',class_ = 'pagenavi').find_all('a')[-2].string
	
#按照分析出的网址变化形式逐页访问
for i in range(1,int(picture_max)):
	href = url + '/' + str(i)	#访问每一页
	response = requests.get(href,headers = header)	#请求数据
	content = response.content	#得到二进制对象
	soup = BeautifulSoup(content,'lxml')	#初始化
		
	#找img标签，访问src属性，找图片url
	picture_url = soup.find('img',alt = title).attrs['src']
	#访问图片url
	response_img = requests.get(picture_url,headers = header)
	#获取二进制图片文件
	content_img = response_img.content
	#命名文件，注意加.jpg
	file_name = dirname + '/'+ title + '-' + str(i) + '.jpg'
	#写入，注意以二进制写入方式打开
	with open(file_name,'wb') as f:
   	 	f.write(content_img)

```

### 准备
* 该程序爬取了[妹子图](https://www.mzitu.com/221136)的44张图片，初学者看懂这个程序需要了解python的模块的引入、参数的定义和设置、循环语句和文件处理，有一定编程基础的可以短时间内学会上述内容，如果有不懂的可以参考我开头放的python教程。
* 这里重点讲述一下模块的引用，程序的开头引入了三个模块，这三个模块是爬虫程序的重要部分，python的强大就在于你可以使用别人的轮子来做非常溜批的操作，安装这三个模块的方法：
```
   pip install requests
   pip install lxml
   pip isntall BeautifulSoup
```
这里请自行搜索pip和这三个模块进行了解。

### 讲解
> 大部分内容我都已经进行了注释，这里讲解部分关键内容。

* 文件夹设置：开头定义了```dirname = 'picture'```，作用是将爬取的图片下载到该文件夹中，如果不设置，图片会直接下载到该程序所在的目录下，会非常的混乱。```file_name = dirname + '/'+ title + '-' + str(i) + '.jpg'```中的```'/'```不能省掉，作用是将图片保存在```..\picture\```中。 
* headers：在请求网页爬取的时候，输出的text信息中会出现抱歉，无法访问等字眼，这就是禁止爬取，需要通过反爬机制去解决这个问题。headers是解决requests请求反爬的方法之一，相当于我们进去这个网页的服务器本身，假装自己本身在爬取数据。查找方法我已经在headers方法后面进行了注释，这里需要一点点的html知识，不过非常简单，相信你一看就能明白。

### 实例
参考该方法我又爬取了另一个[网址](https://www.mzitu.com/201981)的图片，就是身体有点吃不消 :=）

#### 后续我会写爬取其他类型的程序，更新时间看心情吧。希望上述内容可以帮到你，有疑问可以微博或邮箱联系我。

### 补充
* 爬取妹子图网站的原因是我学习的博客爬取的也是这个，后面的实例也是这个原因。
* 我在手机端发现妹子图网站有广告，而且是18禁的那种，所以希望各位在学习的过程中多注意身体，或者使用电脑端（好像没有那种广告）。
* 可能有小朋友问了，“你上来就讲这么多，初次接触python的肯定一头雾水。“我在这里强调一下，在一些稍微有一点点一丢丢复杂的地方我一般都会进行注释，比如放一些其他Blog来让大家更好的理解，所以希望可以把我注释的内容看完，这样才能有更好的理解。
* 我们都不是靠这个吃饭的，单纯是兴趣使然，所以不要纠结于这个程序为什么这样写，别人的那样写。你可以把这个当作一个模板，进行简单修改后就可以爬取其他网站的图片，你要把他当作一个工具，可以用到的工具。
